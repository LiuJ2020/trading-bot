{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Template\n",
    "\n",
    "This notebook helps you develop and test new features for trading strategies.\n",
    "\n",
    "## Workflow:\n",
    "1. Load and explore data\n",
    "2. Create candidate features\n",
    "3. Test stationarity\n",
    "4. Analyze distributions\n",
    "5. Test predictive power\n",
    "6. Document features for production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.append('/Users/jacobliu/repos/projects/trading-bot')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from research.utils import (\n",
    "    load_sample_data,\n",
    "    plot_feature_distributions,\n",
    "    plot_correlation_matrix,\n",
    "    stationarity_test,\n",
    "    distribution_analysis,\n",
    "    feature_importance,\n",
    "    correlation_analysis,\n",
    ")\n",
    "from research.utils.data_loaders import add_features\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = load_sample_data(['SPY', 'QQQ'], days=1000)\n",
    "\n",
    "# Focus on one symbol for feature engineering\n",
    "data_spy = data[data['symbol'] == 'SPY'].set_index('timestamp').sort_index()\n",
    "\n",
    "# Add basic features\n",
    "data_spy = add_features(data_spy, features=['returns', 'sma', 'ema', 'rsi', 'bbands', 'volume_ma'])\n",
    "\n",
    "print(f\"Data shape: {data_spy.shape}\")\n",
    "data_spy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Custom Features\n",
    "\n",
    "Engineer new features based on market intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Momentum features\n",
    "data_spy['momentum_5'] = data_spy['close'].pct_change(5)\n",
    "data_spy['momentum_10'] = data_spy['close'].pct_change(10)\n",
    "data_spy['momentum_20'] = data_spy['close'].pct_change(20)\n",
    "\n",
    "# Volatility features\n",
    "data_spy['vol_5'] = data_spy['returns'].rolling(5).std()\n",
    "data_spy['vol_20'] = data_spy['returns'].rolling(20).std()\n",
    "data_spy['vol_ratio'] = data_spy['vol_5'] / data_spy['vol_20']\n",
    "\n",
    "# Price position features\n",
    "data_spy['price_to_sma50'] = data_spy['close'] / data_spy['sma_50'] - 1\n",
    "data_spy['high_low_range'] = (data_spy['high'] - data_spy['low']) / data_spy['close']\n",
    "\n",
    "# Volume features\n",
    "data_spy['volume_change'] = data_spy['volume'].pct_change()\n",
    "data_spy['volume_sma'] = data_spy['volume'].rolling(20).mean()\n",
    "data_spy['volume_ratio'] = data_spy['volume'] / data_spy['volume_sma']\n",
    "\n",
    "# Trend strength features\n",
    "data_spy['adx'] = calculate_adx(data_spy)  # Average Directional Index\n",
    "data_spy['trend_consistency'] = (data_spy['sma_10'] > data_spy['sma_20']).rolling(10).mean()\n",
    "\n",
    "# Mean reversion features\n",
    "data_spy['z_score'] = (data_spy['close'] - data_spy['sma_20']) / data_spy['close'].rolling(20).std()\n",
    "data_spy['bb_position'] = (data_spy['close'] - data_spy['bb_lower']) / (data_spy['bb_upper'] - data_spy['bb_lower'])\n",
    "\n",
    "# Remove NaN\n",
    "data_spy = data_spy.dropna()\n",
    "\n",
    "print(f\"\\nCreated {data_spy.shape[1]} features\")\n",
    "print(f\"New features: {[col for col in data_spy.columns if col not in ['open', 'high', 'low', 'close', 'volume']]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for ADX calculation\n",
    "def calculate_adx(df, period=14):\n",
    "    \"\"\"Calculate Average Directional Index.\"\"\"\n",
    "    high = df['high']\n",
    "    low = df['low']\n",
    "    close = df['close']\n",
    "    \n",
    "    # True Range\n",
    "    tr1 = high - low\n",
    "    tr2 = abs(high - close.shift())\n",
    "    tr3 = abs(low - close.shift())\n",
    "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "    \n",
    "    # Directional Movement\n",
    "    up = high - high.shift()\n",
    "    down = low.shift() - low\n",
    "    \n",
    "    plus_dm = up.where((up > down) & (up > 0), 0)\n",
    "    minus_dm = down.where((down > up) & (down > 0), 0)\n",
    "    \n",
    "    # Smooth\n",
    "    atr = tr.rolling(period).mean()\n",
    "    plus_di = 100 * (plus_dm.rolling(period).mean() / atr)\n",
    "    minus_di = 100 * (minus_dm.rolling(period).mean() / atr)\n",
    "    \n",
    "    # ADX\n",
    "    dx = 100 * abs(plus_di - minus_di) / (plus_di + minus_di)\n",
    "    adx = dx.rolling(period).mean()\n",
    "    \n",
    "    return adx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features to visualize\n",
    "features_to_plot = [\n",
    "    'returns', 'momentum_20', 'vol_ratio',\n",
    "    'rsi_14', 'z_score', 'bb_position',\n",
    "    'volume_ratio', 'adx', 'trend_consistency'\n",
    "]\n",
    "\n",
    "plot_feature_distributions(data_spy, features=features_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Stationarity\n",
    "\n",
    "Stationary features are generally better for ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test stationarity of key features\n",
    "features_to_test = ['returns', 'momentum_20', 'rsi_14', 'z_score', 'vol_ratio']\n",
    "\n",
    "stationarity_results = {}\n",
    "\n",
    "for feature in features_to_test:\n",
    "    print(f\"\\nTesting: {feature}\")\n",
    "    print(\"=\" * 60)\n",
    "    result = stationarity_test(data_spy[feature], verbose=True)\n",
    "    stationarity_results[feature] = result['is_stationary']\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Stationarity Summary:\")\n",
    "for feature, is_stationary in stationarity_results.items():\n",
    "    status = \"STATIONARY\" if is_stationary else \"NON-STATIONARY\"\n",
    "    print(f\"{feature:20s}: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze distribution of each feature\n",
    "distribution_results = {}\n",
    "\n",
    "for feature in features_to_plot:\n",
    "    results = distribution_analysis(data_spy[feature], name=feature, verbose=True)\n",
    "    distribution_results[feature] = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Correlation Analysis\n",
    "\n",
    "Find features that correlate with future returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target: future returns\n",
    "data_spy['future_returns_1d'] = data_spy['returns'].shift(-1)\n",
    "data_spy['future_returns_5d'] = data_spy['close'].pct_change(5).shift(-5)\n",
    "\n",
    "# Find correlations with 1-day future returns\n",
    "feature_cols = [col for col in data_spy.columns \n",
    "                if col not in ['open', 'high', 'low', 'close', 'volume', \n",
    "                              'returns', 'future_returns_1d', 'future_returns_5d']]\n",
    "\n",
    "print(\"Correlation with 1-day future returns:\")\n",
    "corr_1d = correlation_analysis(\n",
    "    data_spy[feature_cols + ['future_returns_1d']],\n",
    "    target='future_returns_1d',\n",
    "    threshold=0.01\n",
    ")\n",
    "print(corr_1d.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix of top features\n",
    "top_features = corr_1d.head(10)['feature'].tolist() + ['future_returns_1d']\n",
    "plot_correlation_matrix(data_spy[top_features], title='Top Features Correlation Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importance using mutual information\n",
    "X = data_spy[feature_cols]\n",
    "y = data_spy['future_returns_1d'].dropna()\n",
    "X = X.loc[y.index]\n",
    "\n",
    "importance = feature_importance(X, y, method='mutual_info')\n",
    "\n",
    "print(\"\\nTop 15 features by importance:\")\n",
    "for i, (feat, score) in enumerate(list(importance.items())[:15], 1):\n",
    "    print(f\"{i:2d}. {feat:25s}: {score:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "from research.utils.visualization import plot_feature_importance\n",
    "plot_feature_importance(importance, top_n=20, title='Feature Importance (Mutual Information)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Multi-timeframe Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test feature predictive power at different horizons\n",
    "horizons = [1, 2, 5, 10, 20]\n",
    "results = {}\n",
    "\n",
    "for h in horizons:\n",
    "    data_spy[f'future_returns_{h}d'] = data_spy['close'].pct_change(h).shift(-h)\n",
    "    \n",
    "    # Calculate correlation\n",
    "    corr = correlation_analysis(\n",
    "        data_spy[feature_cols + [f'future_returns_{h}d']],\n",
    "        target=f'future_returns_{h}d',\n",
    "        threshold=0.0\n",
    "    )\n",
    "    \n",
    "    results[f'{h}d'] = corr\n",
    "\n",
    "# Compare across horizons\n",
    "print(\"\\nTop feature at each horizon:\")\n",
    "for horizon, corr_df in results.items():\n",
    "    top_feature = corr_df.iloc[0]\n",
    "    print(f\"{horizon:5s}: {top_feature['feature']:25s} (corr={top_feature['correlation']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Interaction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interaction features\n",
    "data_spy['rsi_vol_interaction'] = data_spy['rsi_14'] * data_spy['vol_ratio']\n",
    "data_spy['momentum_vol_interaction'] = data_spy['momentum_20'] * data_spy['vol_ratio']\n",
    "data_spy['rsi_momentum_interaction'] = data_spy['rsi_14'] * data_spy['momentum_20']\n",
    "\n",
    "# Test interactions\n",
    "interaction_features = ['rsi_vol_interaction', 'momentum_vol_interaction', 'rsi_momentum_interaction']\n",
    "\n",
    "interaction_corr = correlation_analysis(\n",
    "    data_spy[interaction_features + ['future_returns_1d']],\n",
    "    target='future_returns_1d',\n",
    "    threshold=0.0\n",
    ")\n",
    "\n",
    "print(\"Interaction feature correlations:\")\n",
    "print(interaction_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Feature Quality Score\n",
    "\n",
    "Score each feature based on multiple criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate quality score for each feature\n",
    "feature_scores = []\n",
    "\n",
    "for feature in feature_cols:\n",
    "    # Criteria 1: Predictive power (correlation)\n",
    "    corr = abs(data_spy[[feature, 'future_returns_1d']].corr().iloc[0, 1])\n",
    "    \n",
    "    # Criteria 2: Stationarity\n",
    "    try:\n",
    "        stat_test = stationarity_test(data_spy[feature], verbose=False)\n",
    "        is_stationary = 1.0 if stat_test['is_stationary'] else 0.0\n",
    "    except:\n",
    "        is_stationary = 0.0\n",
    "    \n",
    "    # Criteria 3: Low correlation with other features (diversity)\n",
    "    feature_corr = data_spy[feature_cols].corr()[feature].drop(feature)\n",
    "    avg_feature_corr = abs(feature_corr).mean()\n",
    "    diversity = 1.0 - min(avg_feature_corr, 1.0)\n",
    "    \n",
    "    # Criteria 4: Low outlier percentage\n",
    "    dist = distribution_analysis(data_spy[feature], verbose=False)\n",
    "    outlier_score = 1.0 - (dist.get('outlier_pct', 0) / 100.0)\n",
    "    \n",
    "    # Combined score (weighted)\n",
    "    score = (\n",
    "        corr * 0.4 +           # Predictive power (40%)\n",
    "        is_stationary * 0.2 +  # Stationarity (20%)\n",
    "        diversity * 0.2 +      # Diversity (20%)\n",
    "        outlier_score * 0.2    # Quality (20%)\n",
    "    )\n",
    "    \n",
    "    feature_scores.append({\n",
    "        'feature': feature,\n",
    "        'score': score,\n",
    "        'correlation': corr,\n",
    "        'stationary': is_stationary,\n",
    "        'diversity': diversity,\n",
    "        'quality': outlier_score,\n",
    "    })\n",
    "\n",
    "feature_scores_df = pd.DataFrame(feature_scores).sort_values('score', ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 features by quality score:\")\n",
    "print(feature_scores_df.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export Feature Definitions\n",
    "\n",
    "Document the best features for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top features\n",
    "top_n = 10\n",
    "top_features_list = feature_scores_df.head(top_n)['feature'].tolist()\n",
    "\n",
    "# Create feature definition export\n",
    "feature_definitions = {\n",
    "    'version': '1.0',\n",
    "    'created_date': str(pd.Timestamp.now().date()),\n",
    "    'target': 'future_returns_1d',\n",
    "    'features': {},\n",
    "}\n",
    "\n",
    "for feature in top_features_list:\n",
    "    feature_info = feature_scores_df[feature_scores_df['feature'] == feature].iloc[0]\n",
    "    \n",
    "    feature_definitions['features'][feature] = {\n",
    "        'quality_score': float(feature_info['score']),\n",
    "        'correlation': float(feature_info['correlation']),\n",
    "        'stationary': bool(feature_info['stationary']),\n",
    "        # Add computation logic here\n",
    "        'computation': 'see notebook',\n",
    "    }\n",
    "\n",
    "import json\n",
    "print(\"\\nFeature Definitions for Production:\")\n",
    "print(json.dumps(feature_definitions, indent=2))\n",
    "\n",
    "# Optionally save to file\n",
    "# with open('feature_definitions_v1.json', 'w') as f:\n",
    "#     json.dump(feature_definitions, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### Key Findings:\n",
    "1. List the most important features\n",
    "2. Note which features are stationary\n",
    "3. Identify feature interactions\n",
    "4. Document any surprises or insights\n",
    "\n",
    "### Next Steps:\n",
    "1. Implement top features in the feature store\n",
    "2. Test features in strategy development\n",
    "3. Monitor feature distributions in production\n",
    "4. Set up drift detection alerts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
